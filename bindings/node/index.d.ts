/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

/** Model loading parameters */
export interface JsModelParams {
  /** Number of layers to offload to GPU (0 = CPU only, -1 = all) */
  nGpuLayers?: number
  /** Use memory mapping for model loading */
  useMmap?: boolean
  /** Lock model in memory */
  useMlock?: boolean
  /** Only load vocabulary (for tokenization only) */
  vocabOnly?: boolean
}
/** Context creation parameters */
export interface JsContextParams {
  /** Context size (0 = use model default) */
  nCtx?: number
  /** Batch size for prompt processing */
  nBatch?: number
  /** Number of threads (0 = auto) */
  nThreads?: number
  /** Enable embeddings mode */
  embeddings?: boolean
}
/** Sampler parameters for text generation */
export interface JsSamplerParams {
  /** Temperature (0.0 = deterministic, higher = more random) */
  temperature?: number
  /** Top-k sampling (0 = disabled) */
  topK?: number
  /** Top-p/nucleus sampling (1.0 = disabled) */
  topP?: number
  /** Min-p sampling (0.0 = disabled) */
  minP?: number
  /** Typical sampling (1.0 = disabled) */
  typicalP?: number
  /** Repeat penalty (1.0 = disabled) */
  penaltyRepeat?: number
  /** Frequency penalty (0.0 = disabled) */
  penaltyFreq?: number
  /** Presence penalty (0.0 = disabled) */
  penaltyPresent?: number
  /** Tokens to consider for penalties */
  penaltyLastN?: number
  /** Random seed (0 = random) */
  seed?: number
}
/** Greedy sampler parameters (deterministic) */
export declare function samplerParamsGreedy(): JsSamplerParams
/** Creative sampler parameters (high randomness) */
export declare function samplerParamsCreative(): JsSamplerParams
/** Precise sampler parameters (low randomness) */
export declare function samplerParamsPrecise(): JsSamplerParams
/** Compute cosine similarity between two vectors */
export declare function cosineSimilarity(a: Array<number>, b: Array<number>): number
/** Initialize the mullama backend */
export declare function backendInit(): void
/** Free the mullama backend resources */
export declare function backendFree(): void
/** Check if GPU offloading is supported */
export declare function supportsGpuOffload(): boolean
/** Get system information */
export declare function systemInfo(): string
/** Get the maximum number of supported devices */
export declare function maxDevices(): number
/** Get the library version */
export declare function version(): string
/** Model class for loading and managing LLM models */
export declare class JsModel {
  /** Load a model from a GGUF file */
  static load(path: string, params?: JsModelParams | undefined | null): JsModel
  /** Tokenize text into token IDs */
  tokenize(text: string, addBos?: boolean | undefined | null, special?: boolean | undefined | null): Array<number>
  /** Detokenize token IDs back to text */
  detokenize(tokens: Array<number>, removeSpecial?: boolean | undefined | null, unparseSpecial?: boolean | undefined | null): string
  /** Get the model's training context size */
  get nCtxTrain(): number
  /** Get the model's embedding dimension */
  get nEmbd(): number
  /** Get the vocabulary size */
  get nVocab(): number
  /** Get the number of layers */
  get nLayer(): number
  /** Get the number of attention heads */
  get nHead(): number
  /** Get the BOS (beginning of sequence) token ID */
  get tokenBos(): number
  /** Get the EOS (end of sequence) token ID */
  get tokenEos(): number
  /** Get the model size in bytes */
  get size(): number
  /** Get the number of parameters */
  get nParams(): number
  /** Get the model description */
  get description(): string
  /** Get the model architecture */
  get architecture(): string | null
  /** Get the model name from metadata */
  get name(): string | null
  /** Check if a token is end-of-generation */
  tokenIsEog(token: number): boolean
  /** Get all metadata as an object */
  metadata(): Record<string, string>
  /** Apply chat template to format messages */
  applyChatTemplate(messages: Array<[string, string]>, addGenerationPrompt?: boolean | undefined | null): string
}
/** Context for model inference */
export declare class JsContext {
  /** Create a new context from a model */
  constructor(model: JsModel, params?: JsContextParams | undefined | null)
  /** Generate text from a prompt */
  generate(prompt: string, maxTokens?: number | undefined | null, params?: JsSamplerParams | undefined | null): string
  /** Generate text from token IDs */
  generateFromTokens(tokens: Array<number>, maxTokens?: number | undefined | null, params?: JsSamplerParams | undefined | null): string
  /** Generate text with streaming (returns array of tokens) */
  generateStream(prompt: string, maxTokens?: number | undefined | null, params?: JsSamplerParams | undefined | null): Array<string>
  /** Clear the KV cache */
  clearCache(): void
  /** Get the context size */
  get nCtx(): number
  /** Get the batch size */
  get nBatch(): number
  /** Get embeddings (if embeddings mode is enabled) */
  getEmbeddings(): Array<number> | null
}
/** Embedding generator for creating text embeddings */
export declare class JsEmbeddingGenerator {
  /** Create a new embedding generator */
  constructor(model: JsModel, nCtx?: number | undefined | null, normalize?: boolean | undefined | null)
  /** Generate embeddings for text */
  embed(text: string): Array<number>
  /** Generate embeddings for multiple texts */
  embedBatch(texts: Array<string>): Array<Array<number>>
  /** Get the embedding dimension */
  get nEmbd(): number
}
