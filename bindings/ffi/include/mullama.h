/*
 * Mullama FFI - C API for Mullama LLM Library
 *
 * This header provides the C ABI for integrating Mullama into
 * Node.js, Python, PHP, Go, and other languages.
 *
 * Copyright (c) 2025-2026 Mullama Contributors
 * Licensed under MIT OR Apache-2.0
 */


#ifndef MULLAMA_FFI_H
#define MULLAMA_FFI_H

/* Generated with cbindgen:0.26.0 */

/* Warning: this file is autogenerated by cbindgen. Do not modify manually. */

#include <stdarg.h>
#include <stdbool.h>
#include <stddef.h>
#include <stdint.h>
#include <stdlib.h>
// Atomic bool type for cancellation tokens (implementation detail)
typedef struct { volatile int value; } MullamaAtomicBool;


// Library version major number
#define MullamaMULLAMA_VERSION_MAJOR 0

// Library version minor number
#define MullamaMULLAMA_VERSION_MINOR 1

// Library version patch number
#define MullamaMULLAMA_VERSION_PATCH 0

// A wrapper for creating opaque handles that can cross FFI boundaries.
//
// Handles are created by boxing an Arc, which ensures:
// 1. The data lives as long as the handle exists
// 2. Multiple handles can safely reference the same data
// 3. Proper cleanup when the last handle is freed
typedef struct MullamaHandle_Model MullamaHandle_Model;

// A mutable handle for types that need interior mutability.
//
// Uses a Mutex to provide safe mutable access across FFI.
typedef struct MullamaMutableHandle_Context MullamaMutableHandle_Context;

// A mutable handle for types that need interior mutability.
//
// Uses a Mutex to provide safe mutable access across FFI.
typedef struct MullamaMutableHandle_EmbeddingGeneratorInner MullamaMutableHandle_EmbeddingGeneratorInner;

// Context creation parameters
typedef struct MullamaMullamaContextParams {
  // Context size (0 = use model default)
  uint32_t n_ctx;
  // Logical batch size for prompt processing
  uint32_t n_batch;
  // Physical batch size (memory allocation)
  uint32_t n_ubatch;
  // Maximum number of sequences
  uint32_t n_seq_max;
  // Number of threads for generation
  int n_threads;
  // Number of threads for batch processing
  int n_threads_batch;
  // Enable embeddings mode
  bool embeddings;
  // Offload KQV to GPU
  bool offload_kqv;
  // Flash attention type (0=auto, 1=disabled, 2=enabled)
  int flash_attn;
} MullamaMullamaContextParams;

// Opaque context handle for FFI
typedef struct MullamaMutableHandle_Context MullamaMullamaContext;

// Opaque model handle for FFI
typedef struct MullamaHandle_Model MullamaMullamaModel;

// Sampler parameters for text generation
typedef struct MullamaMullamaSamplerParams {
  // Temperature for randomness (0.0 = deterministic, higher = more random)
  float temperature;
  // Top-k sampling (0 = disabled)
  int top_k;
  // Top-p (nucleus) sampling (1.0 = disabled)
  float top_p;
  // Min-p sampling (0.0 = disabled)
  float min_p;
  // Typical sampling (1.0 = disabled)
  float typical_p;
  // Repeat penalty (1.0 = disabled)
  float penalty_repeat;
  // Frequency penalty (0.0 = disabled)
  float penalty_freq;
  // Presence penalty (0.0 = disabled)
  float penalty_present;
  // Number of last tokens to consider for penalty
  int penalty_last_n;
  // Whether to penalize newline tokens
  bool penalize_nl;
  // Whether to ignore EOS tokens
  bool ignore_eos;
  // Random seed (0 = random)
  uint32_t seed;
} MullamaMullamaSamplerParams;

// Configuration for embedding generation
typedef struct MullamaMullamaEmbeddingConfig {
  // Context size (0 = use model default)
  uint32_t n_ctx;
  // Batch size for processing
  uint32_t n_batch;
  // Number of threads
  int n_threads;
  // Pooling type: 0=none, 1=mean, 2=cls
  int pooling_type;
  // Normalize embeddings
  bool normalize;
} MullamaMullamaEmbeddingConfig;

typedef struct MullamaMutableHandle_EmbeddingGeneratorInner MullamaMullamaEmbeddingGenerator;

// Model loading parameters
typedef struct MullamaMullamaModelParams {
  // Number of layers to offload to GPU (0 = CPU only, -1 = all)
  int n_gpu_layers;
  // Main GPU device index (for multi-GPU systems)
  int main_gpu;
  // Use memory mapping for model loading
  bool use_mmap;
  // Lock model in memory
  bool use_mlock;
  // Only load vocabulary (for tokenization only)
  bool vocab_only;
  // Check tensor data integrity
  bool check_tensors;
} MullamaMullamaModelParams;

// Apply a chat template to format messages
//
// # Arguments
// * `model` - Model handle
// * `messages` - Array of chat messages
// * `n_messages` - Number of messages
// * `add_generation_prompt` - Whether to add generation prompt
// * `output` - Output buffer
// * `max_output` - Output buffer size
//
// # Returns
// Number of bytes written, or negative error code.
typedef struct MullamaMullamaChatMessage {
  const char *role;
  const char *content;
} MullamaMullamaChatMessage;

// Callback function type for streaming token generation
//
// # Arguments
// * `token` - The generated token text (null-terminated UTF-8)
// * `user_data` - User-provided data pointer
//
// # Returns
// Return `true` to continue generation, `false` to stop.
typedef bool (*MullamaMullamaStreamCallback)(const char *token, void *user_data);

// Cancellation token for streaming operations
typedef struct MullamaMullamaCancelToken {
  MullamaAtomicBool cancelled;
} MullamaMullamaCancelToken;

#ifdef __cplusplus
extern "C" {
#endif // __cplusplus

// Initialize the Mullama/llama.cpp backend
//
// This should be called once before using any other functions.
// It is safe to call multiple times.
void mullama_backend_init(void);

// Free the Mullama/llama.cpp backend resources
//
// Call this when completely done with the library.
void mullama_backend_free(void);

// Check if GPU offloading is supported
bool mullama_supports_gpu_offload(void);

// Check if memory mapping is supported
bool mullama_supports_mmap(void);

// Check if memory locking is supported
bool mullama_supports_mlock(void);

// Get maximum number of devices supported
size_t mullama_max_devices(void);

// Get system information string
//
// # Arguments
// * `output` - Output buffer
// * `max_output` - Size of output buffer
//
// # Returns
// Number of bytes written, or negative required size
int mullama_system_info(char *output, size_t max_output);

// Get library version as a string
const char *mullama_version(void);

// Get library version major number
uint32_t mullama_version_major(void);

// Get library version minor number
uint32_t mullama_version_minor(void);

// Get library version patch number
uint32_t mullama_version_patch(void);

// Get current timestamp in microseconds
int64_t mullama_time_us(void);

// Get default context parameters
struct MullamaMullamaContextParams mullama_context_default_params(void);

// Create a new context from a model
//
// # Arguments
// * `model` - Model handle
// * `params` - Optional context parameters (NULL for defaults)
//
// # Returns
// A context handle on success, or NULL on failure.
MullamaMullamaContext *mullama_context_new(const MullamaMullamaModel *model,
                                           const struct MullamaMullamaContextParams *params);

// Free a context handle
void mullama_context_free(MullamaMullamaContext *ctx);

// Decode tokens (process them through the model)
//
// # Arguments
// * `ctx` - Context handle
// * `tokens` - Token IDs to decode
// * `n_tokens` - Number of tokens
//
// # Returns
// 0 on success, negative error code on failure.
int mullama_decode(MullamaMullamaContext *ctx, const int *tokens, int n_tokens);

// Generate text from prompt tokens
//
// # Arguments
// * `ctx` - Context handle
// * `tokens` - Prompt token IDs
// * `n_tokens` - Number of prompt tokens
// * `max_tokens` - Maximum tokens to generate
// * `params` - Optional sampler parameters (NULL for defaults)
// * `output` - Output buffer for generated text
// * `max_output` - Size of output buffer
//
// # Returns
// Number of bytes written on success, or negative error code.
int mullama_generate(MullamaMullamaContext *ctx,
                     const int *tokens,
                     int n_tokens,
                     int max_tokens,
                     const struct MullamaMullamaSamplerParams *params,
                     char *output,
                     size_t max_output);

// Get the context size
uint32_t mullama_context_n_ctx(const MullamaMullamaContext *ctx);

// Get the batch size
uint32_t mullama_context_n_batch(const MullamaMullamaContext *ctx);

// Clear the KV cache
int mullama_context_kv_cache_clear(MullamaMullamaContext *ctx);

// Remove tokens from KV cache for a sequence
//
// # Arguments
// * `ctx` - Context handle
// * `seq_id` - Sequence ID (-1 for all sequences)
// * `p0` - Start position (inclusive)
// * `p1` - End position (exclusive)
//
// # Returns
// 1 on success, 0 on failure.
int mullama_context_kv_cache_seq_rm(MullamaMullamaContext *ctx, int seq_id, int p0, int p1);

// Get the number of threads used for generation
int mullama_context_n_threads(const MullamaMullamaContext *ctx);

// Set the number of threads
int mullama_context_set_n_threads(MullamaMullamaContext *ctx, int n_threads, int n_threads_batch);

// Get logits for the last token
//
// # Arguments
// * `ctx` - Context handle
// * `output` - Output buffer for logits
// * `max_output` - Size of output buffer (should be >= n_vocab)
//
// # Returns
// Number of floats written, or negative error code.
int mullama_context_get_logits(const MullamaMullamaContext *ctx, float *output, size_t max_output);

// Get embeddings for the last token (requires embeddings mode)
//
// # Arguments
// * `ctx` - Context handle
// * `output` - Output buffer for embeddings
// * `max_output` - Size of output buffer (should be >= n_embd)
//
// # Returns
// Number of floats written, or negative error code.
int mullama_context_get_embeddings(const MullamaMullamaContext *ctx,
                                   float *output,
                                   size_t max_output);

// Save context state to a buffer
//
// # Arguments
// * `ctx` - Context handle
// * `output` - Output buffer
// * `max_output` - Size of output buffer
//
// # Returns
// Number of bytes written, or negative required size.
int mullama_context_save_state(const MullamaMullamaContext *ctx,
                               uint8_t *output,
                               size_t max_output);

// Load context state from a buffer
//
// # Arguments
// * `ctx` - Context handle
// * `data` - State data buffer
// * `data_size` - Size of state data
//
// # Returns
// Number of bytes read, or negative error code.
int mullama_context_load_state(MullamaMullamaContext *ctx, const uint8_t *data, size_t data_size);

// Get default embedding configuration
struct MullamaMullamaEmbeddingConfig mullama_embedding_default_config(void);

// Create a new embedding generator
//
// # Arguments
// * `model` - Model handle
// * `config` - Optional configuration (NULL for defaults)
//
// # Returns
// An embedding generator handle on success, or NULL on failure.
MullamaMullamaEmbeddingGenerator *mullama_embedding_generator_new(const MullamaMullamaModel *model,
                                                                  const struct MullamaMullamaEmbeddingConfig *config);

// Free an embedding generator handle
void mullama_embedding_generator_free(MullamaMullamaEmbeddingGenerator *gen);

// Generate embeddings for text
//
// # Arguments
// * `gen` - Embedding generator handle
// * `text` - Text to embed
// * `output` - Output buffer for embeddings
// * `max_output` - Size of output buffer (should be >= n_embd)
//
// # Returns
// Number of floats written on success, or negative error code.
int mullama_embed_text(MullamaMullamaEmbeddingGenerator *gen,
                       const char *text,
                       float *output,
                       size_t max_output);

// Generate embeddings for multiple texts in a batch
//
// # Arguments
// * `gen` - Embedding generator handle
// * `texts` - Array of text pointers
// * `n_texts` - Number of texts
// * `output` - Output buffer for embeddings (flattened: n_texts * n_embd)
// * `max_output` - Size of output buffer
//
// # Returns
// Number of floats written on success (n_texts * n_embd), or negative error code.
int mullama_embed_batch(MullamaMullamaEmbeddingGenerator *gen,
                        const char *const *texts,
                        int n_texts,
                        float *output,
                        size_t max_output);

// Get the embedding dimension for the generator
int mullama_embedding_generator_n_embd(const MullamaMullamaEmbeddingGenerator *gen);

// Compute cosine similarity between two embedding vectors
//
// # Arguments
// * `a` - First embedding vector
// * `b` - Second embedding vector
// * `n` - Dimension of the vectors
//
// # Returns
// Cosine similarity value between -1 and 1
float mullama_embedding_cosine_similarity(const float *a, const float *b, size_t n);

// Normalize an embedding vector in-place
//
// # Arguments
// * `embedding` - Embedding vector to normalize
// * `n` - Dimension of the vector
void mullama_embedding_normalize(float *embedding, size_t n);

// Get a pointer to the last error message for the current thread.
//
// Returns null if no error has been set. The returned pointer is valid
// until the next FFI call on the same thread.
//
// # Safety
// This function is safe to call, but the returned pointer should not
// be stored long-term as it may be invalidated by subsequent FFI calls.
const char *mullama_get_last_error(void);

// Clear the last error message.
void mullama_clear_error(void);

// Get a human-readable description of an error code.
const char *mullama_error_code_description(int32_t code);

// Get default model parameters
struct MullamaMullamaModelParams mullama_model_default_params(void);

// Load a model from a GGUF file
//
// # Arguments
// * `path` - Path to the GGUF model file
// * `params` - Optional model parameters (NULL for defaults)
//
// # Returns
// A model handle on success, or NULL on failure.
// Check `mullama_get_last_error()` for error details.
MullamaMullamaModel *mullama_model_load(const char *path,
                                        const struct MullamaMullamaModelParams *params);

// Free a model handle
//
// # Safety
// The model handle must have been created by `mullama_model_load`.
// After calling this function, the handle is invalid.
void mullama_model_free(MullamaMullamaModel *model);

// Tokenize text into token IDs
//
// # Arguments
// * `model` - Model handle
// * `text` - Text to tokenize
// * `tokens` - Output buffer for tokens
// * `max_tokens` - Size of the output buffer
// * `add_bos` - Whether to add beginning-of-sequence token
// * `special` - Whether to parse special tokens
//
// # Returns
// Number of tokens written on success, or negative error code on failure.
// If the buffer is too small, returns the required size as a negative number.
int mullama_tokenize(const MullamaMullamaModel *model,
                     const char *text,
                     int *tokens,
                     int max_tokens,
                     bool add_bos,
                     bool special);

// Detokenize tokens back to text
//
// # Arguments
// * `model` - Model handle
// * `tokens` - Token IDs to detokenize
// * `n_tokens` - Number of tokens
// * `output` - Output buffer for text
// * `max_output` - Size of the output buffer
//
// # Returns
// Number of bytes written on success, or negative error code on failure.
// If the buffer is too small, returns the required size as a negative number.
int mullama_detokenize(const MullamaMullamaModel *model,
                       const int *tokens,
                       int n_tokens,
                       char *output,
                       int max_output);

// Get the model's training context size
int mullama_model_n_ctx_train(const MullamaMullamaModel *model);

// Get the model's embedding dimension
int mullama_model_n_embd(const MullamaMullamaModel *model);

// Get the model's vocabulary size
int mullama_model_n_vocab(const MullamaMullamaModel *model);

// Get the number of layers in the model
int mullama_model_n_layer(const MullamaMullamaModel *model);

// Get the number of attention heads
int mullama_model_n_head(const MullamaMullamaModel *model);

// Get the BOS (beginning of sequence) token ID
int mullama_model_token_bos(const MullamaMullamaModel *model);

// Get the EOS (end of sequence) token ID
int mullama_model_token_eos(const MullamaMullamaModel *model);

// Get the newline token ID
int mullama_model_token_nl(const MullamaMullamaModel *model);

// Check if a token is end-of-generation
bool mullama_model_token_is_eog(const MullamaMullamaModel *model, int token);

// Get the model description
//
// # Returns
// Number of bytes written, or negative error code.
int mullama_model_desc(const MullamaMullamaModel *model, char *output, size_t max_output);

// Get the model size in bytes
uint64_t mullama_model_size(const MullamaMullamaModel *model);

// Get the number of parameters in the model
uint64_t mullama_model_n_params(const MullamaMullamaModel *model);

// Check if model has an encoder (encoder-decoder models)
bool mullama_model_has_encoder(const MullamaMullamaModel *model);

// Check if model has a decoder
bool mullama_model_has_decoder(const MullamaMullamaModel *model);

// Clone the model Arc (increases reference count)
//
// Returns a new handle that shares ownership with the original.
MullamaMullamaModel *mullama_model_clone(const MullamaMullamaModel *model);

// Get a metadata value from the model by key
//
// # Returns
// Number of bytes written, or negative error code.
int mullama_model_meta_val(const MullamaMullamaModel *model,
                           const char *key,
                           char *output,
                           size_t max_output);

int mullama_model_apply_chat_template(const MullamaMullamaModel *model,
                                      const struct MullamaMullamaChatMessage *messages,
                                      int n_messages,
                                      bool add_generation_prompt,
                                      char *output,
                                      size_t max_output);

// Get default sampler parameters
struct MullamaMullamaSamplerParams mullama_sampler_default_params(void);

// Create greedy sampler params (deterministic, always picks top token)
struct MullamaMullamaSamplerParams mullama_sampler_greedy_params(void);

// Create creative sampler params (higher randomness)
struct MullamaMullamaSamplerParams mullama_sampler_creative_params(void);

// Create precise sampler params (lower randomness, more focused)
struct MullamaMullamaSamplerParams mullama_sampler_precise_params(void);

// Generate text with streaming callback
//
// # Arguments
// * `ctx` - Context handle
// * `tokens` - Prompt token IDs
// * `n_tokens` - Number of prompt tokens
// * `max_tokens` - Maximum tokens to generate
// * `params` - Optional sampler parameters (NULL for defaults)
// * `callback` - Callback function for each generated token
// * `user_data` - User data to pass to callback
//
// # Returns
// Number of tokens generated on success, or negative error code.
int mullama_generate_streaming(MullamaMullamaContext *ctx,
                               const int *tokens,
                               int n_tokens,
                               int max_tokens,
                               const struct MullamaMullamaSamplerParams *params,
                               MullamaMullamaStreamCallback callback,
                               void *user_data);

// Create a new cancellation token
struct MullamaMullamaCancelToken *mullama_cancel_token_new(void);

// Cancel the operation associated with this token
void mullama_cancel_token_cancel(struct MullamaMullamaCancelToken *token);

// Check if the token has been cancelled
bool mullama_cancel_token_is_cancelled(const struct MullamaMullamaCancelToken *token);

// Free a cancellation token
void mullama_cancel_token_free(struct MullamaMullamaCancelToken *token);

// Generate text with streaming callback and cancellation support
//
// # Arguments
// * `ctx` - Context handle
// * `tokens` - Prompt token IDs
// * `n_tokens` - Number of prompt tokens
// * `max_tokens` - Maximum tokens to generate
// * `params` - Optional sampler parameters (NULL for defaults)
// * `callback` - Callback function for each generated token
// * `user_data` - User data to pass to callback
// * `cancel_token` - Optional cancellation token (NULL to ignore)
//
// # Returns
// Number of tokens generated on success, or negative error code.
// Returns MULLAMA_ERR_CANCELLED if generation was cancelled.
int mullama_generate_streaming_cancellable(MullamaMullamaContext *ctx,
                                           const int *tokens,
                                           int n_tokens,
                                           int max_tokens,
                                           const struct MullamaMullamaSamplerParams *params,
                                           MullamaMullamaStreamCallback callback,
                                           void *user_data,
                                           const struct MullamaMullamaCancelToken *cancel_token);

#ifdef __cplusplus
} // extern "C"
#endif // __cplusplus

#endif /* MULLAMA_FFI_H */
